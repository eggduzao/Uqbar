{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dabe130-8f77-4e6d-87cb-244596448bc9",
   "metadata": {},
   "source": [
    "# OpenRouter vs Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d65cc-0c3c-42e7-a10e-b2f55ac0c085",
   "metadata": {},
   "source": [
    "## Open Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6f8d3-f44f-4cba-8b7e-272f8df1226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "export OPENROUTER_API_KEY=\"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\"\n",
    "python your_file.py\n",
    "\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short story about a robot learning to love.\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcdcee-082f-4fa9-8db8-88f88e249760",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLEType = Literal[\"system\", \"user\", \"assistant\", \"developer\"]\n",
    "\n",
    "DEFAULT_MODEL: str = \"allenai/olmo-3.1-32b-think:free\"\n",
    "DEFAULT_ROLE: ROLEType = \"user\"\n",
    "DEFAULT_BASE_URL: str = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "DEFAULT_HEADERS: dict[str, str] = {\n",
    "    \"HTTP-Referer\": \"https://www.gusmaolab.org\",\n",
    "    \"X-Title\": \"Acta Diurna\",\n",
    "}\n",
    "\n",
    "DEFAULT_EXTRA_BODY: dict[str, str] = {\"user\": \"acta-local-test\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7bb36-cd49-4f64-a19f-5f52eeb99c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtnow() -> str:\n",
    "    \"\"\"Return an ISO-8601 UTC timestamp string.\"\"\"\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "\n",
    "def _merge_headers(*parts: dict[str, str] | None) -> dict[str, str]:\n",
    "    \"\"\"Merge multiple header dicts (later dicts override earlier ones).\"\"\"\n",
    "    out: dict[str, str] = {}\n",
    "    for p in parts:\n",
    "        if p:\n",
    "            out.update(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _perform_query_trend(\n",
    "    *,\n",
    "    query: str,\n",
    "    model: str,\n",
    "    role: ROLEType,\n",
    "    base_url: str,\n",
    "    apkey: str,\n",
    "    default_headers: dict[str, str] | None = None,\n",
    "    extra_body: dict[str, Any] | None = None,\n",
    "    app_url: str | None = None,\n",
    "    app_title: str | None = None,\n",
    "    temperature: float | None = None,\n",
    "    top_p: float | None = None,\n",
    "    max_tokens: int | None = None,\n",
    "    seed: int | None = None,\n",
    "    timeout_s: float | None = 120.0,\n",
    "    max_retries: int = 3,\n",
    "    system_prompt: str | None = None,\n",
    ") -> ChatCompletion | None:\n",
    "    \"\"\"Perform a single chat completion request. Returns None on failure.\"\"\"\n",
    "    if not query.strip():\n",
    "        print(f\"{dtnow()} ERROR: query must be a non-empty string.\")\n",
    "        return None\n",
    "\n",
    "    attribution_headers: dict[str, str] = {}\n",
    "    if app_url:\n",
    "        attribution_headers[\"HTTP-Referer\"] = app_url\n",
    "    if app_title:\n",
    "        attribution_headers[\"X-Title\"] = app_title\n",
    "\n",
    "    headers = _merge_headers(default_headers, attribution_headers)\n",
    "\n",
    "    client = OpenAI(base_url=base_url, api_key=apkey, default_headers=headers)\n",
    "    try:\n",
    "        client = client.with_options(max_retries=max_retries, timeout=timeout_s)\n",
    "    except TypeError:\n",
    "        # Older SDK: ignore\n",
    "        pass\n",
    "\n",
    "    messages: list[dict[str, str]] = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": role, \"content\": query})\n",
    "\n",
    "    payload: dict[str, Any] = {\"model\": model, \"messages\": messages}\n",
    "\n",
    "    if temperature is not None:\n",
    "        payload[\"temperature\"] = float(temperature)\n",
    "    if top_p is not None:\n",
    "        payload[\"top_p\"] = float(top_p)\n",
    "    if max_tokens is not None:\n",
    "        payload[\"max_tokens\"] = int(max_tokens)\n",
    "    if seed is not None:\n",
    "        payload[\"seed\"] = int(seed)\n",
    "\n",
    "    if extra_body:\n",
    "        payload.update(extra_body)\n",
    "\n",
    "    try:\n",
    "        return client.chat.completions.create(**payload)\n",
    "    except Exception as e:\n",
    "        print(f\"{dtnow()} ERROR: request failed for model={model!r}\")\n",
    "        print(f\"{dtnow()} {type(e).__name__}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b28cee-f76c-496c-8d5e-cb20631406b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_models(\n",
    "    *,\n",
    "    query: str,\n",
    "    output_path: Path,\n",
    "    apkey: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    role: ROLEType = DEFAULT_ROLE,\n",
    "    base_url: str = DEFAULT_BASE_URL,\n",
    "    default_headers: dict[str, str] = DEFAULT_HEADERS,\n",
    "    default_extra_body: dict[str, str] = DEFAULT_EXTRA_BODY,\n",
    ") -> int:\n",
    "    \"\"\"Run one query and dump choices to a file. Returns 0 on success, 1 on failure.\"\"\"\n",
    "    response = _perform_query_trend(\n",
    "        query=query,\n",
    "        model=model,\n",
    "        role=role,\n",
    "        base_url=base_url,\n",
    "        apkey=apkey,\n",
    "        default_headers=default_headers,\n",
    "        extra_body=default_extra_body,\n",
    "        app_url=default_headers.get(\"HTTP-Referer\"),\n",
    "        app_title=default_headers.get(\"X-Title\"),\n",
    "    )\n",
    "\n",
    "    if response is None or not getattr(response, \"choices\", None):\n",
    "        print(f\"{dtnow()} ERROR: no response/choices\")\n",
    "        return 1\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            content = getattr(getattr(choice, \"message\", None), \"content\", None)\n",
    "            f.write(f\"#################### response.choices[{idx}]\\n\")\n",
    "            f.write(f\"# Content:\\n{content}\\n\")\n",
    "            f.write(f\"# Message:\\n{choice.message}\\n\")\n",
    "            f.write(f\"{'='*100}\\n\\n\")\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79278103-0a6f-40f0-898e-1a3eaa91e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\", \"\").strip()\n",
    "if not api_key:\n",
    "    raise SystemExit(\"Set OPENROUTER_API_KEY in your environment first.\")\n",
    "\n",
    "query = (\n",
    "    \"Who is Einstein? Please answer inside a triple-backtick codebox with\"\n",
    "    \"a maximum of ~1-3 line(s) [not counting the triple backticks in the total length].\"\n",
    ")\n",
    "output_path = Path.home() / \"tmp\" / \"openrouter_result.txt\"\n",
    "\n",
    "raise SystemExit(\n",
    "    query_models(query=query, output_path=output_path, apkey=api_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be26da8-61a1-4604-90ed-73196613e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2518a-81ce-4d1e-b568-f5855ff2d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "See huggingface.txt in Organization\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7c475-b81e-4e37-ba2e-30b2088b9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a short story about a robot learning to love.\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438b0cc-bf73-45ed-9cb1-845f4e8281f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"together\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "# output is a PIL.Image object\n",
    "image = client.text_to_image(\n",
    "    \"A steampunk airship in the clouds\",\n",
    "    model=\"black-forest-labs/FLUX.1-dev\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e43cc8-84ef-4b83-afb6-863623c944fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/GLM-4.7-Flash-4bit\")\n",
    "\n",
    "prompt = \"hello\"\n",
    "\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_dict=False,\n",
    "    )\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdd5c8-3273-4740-8b38-67db83907bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_id = \"lmstudio-community/Qwen2.5-7B-Instruct-MLX-4bit\"\n",
    "model, tokenizer = load(model_id)\n",
    "\n",
    "prompt = \"Write a 1200-word news-style story about a discovery. Then give 100-word summary, 5 keywords, and 1 mood word.\"\n",
    "text = generate(model, tokenizer, prompt=prompt, max_tokens=1800)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d7053-058c-4b9e-a636-1916baf523b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run as a command in shell\n",
    "\"\"\"\n",
    "mlx_lm.generate \\\n",
    "  --model mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit \\\n",
    "  --prompt \"Say 'hi'.\" \\\n",
    "  --max-tokens 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b52ea-a00f-4c27-a411-b73c2b4398d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/DeepSeek-R1-Distill-Qwen-32B-4bit\")\n",
    "\n",
    "prompt=\"hello\"\n",
    "\n",
    "if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
